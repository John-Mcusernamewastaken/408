- TODO.
	- Write Final Report.
		- Abstract.
		- Precise Specification of Goals.
		- Prior Work.
		- Software Specfication.
		- Details of Implementation.
		- Method.
		- Results.
		- Reflection on Development Process, Sprints.
	- Clarify MDP Definition.
	- Convert TTT to support arbitary size.
	- Garbage.
		- Advantages of off-policy vs on-policy.
		- Reading.
		- Policy-Gradient.
			- Asynchronous methods for deep reinforcement learning (A2C).
			- Proximal Policy Optimization.
		- Understand & implement Actor-Critic.
		- Find some other improvements from RAINBOW, understand & implement.
		- C51 algorithm.
- Goals for 21.12.23 Meeting.
	- Questions.
		- Progress Update.
			- Implemented a maze environment (agents suck at it).
			- Implemented DQN w/ experience replay.
			- Been reading papers (still); TD-Gammon, DQN-Atari, Actor-Critic methods (but feel like I haven't grasped it).
			- Started work on an Actor-Critic implementation, but haven't finished it.
		- How much maths should I be including?
			- I'm planning to include...
			- Description of a Markov Decision Process.
			- Learning rules/loss functions for all the algorithms I'm implementing.
			- How do I properly attribute these? I read online that it's fine to reproduce equations without citation, but this seems like a special case.
			
			- More on the settings of the experiments.
		- Totally new methods or gradient policy methods (quote the paper in the literature review).
		- In the background when you introduce the experiments simply namedrop.
		- Focus on the background.
- Goals for 25.2.24 Meeting.
	- Progress Update.
		- Been reading about policy entropy.
	- Questions.
		- Algorithm Shortlist.
			- Q-Val.
				- DQN.
				- SARSA.
			- Policy Gradient.
				- Vanilla Policy Gradient.
				- PPO.
			- Actor-Critic.
				- Vanilla Actor-Critic.
				- A2C + .
				- Soft Actor-Critic.
			- "DQN, SARSA, Vanilla Policy Gradient (REINFORCE), PPO, Vanilla Actor-Critic, Advantage Actor-Critic."
- Goals for 8.2.24 Meeting.
	- Progress Update.
		- Spent a few days implementing. 
		- Finalized environment list.
	- Thoughts.
		- Resources very limited.
		- Do not convolutional.
- Async deep learning paper graphs (edit: maybe I meant table?) max of agents trained at different learning rates, could do that.
- Is Q-learning appropriate?
	- Yes.
	- Compare with & without various improvements?
		- Experience Replay (Atari Deep Learning, Minh et al, 2013).
		- Rainbow stuff (Rainbow: Combining Improvements in Deep Reinforcement Learning, Hessel et al, 2017).
	- PPO paper says it's bad for continuous control.
- Value-Based Methods (Dynamic Programming/Monte-Carlo/Temporal Difference).
	- These are just strategies that are used with specific algorithms i.e. Q-Learning is temporal difference, REINFORCE is TD (I think?).
	- TD Methods.
		- TD-0.
		- SARSA (agrees with Sutton & Barto).
			- Like Q-learning but on-policy.
		- Q-Learning (agrees with Sutton & Barto).
			- Like SARSA but off-policy.
- Actor-Critic.
	- Soft-Actor-Critic vs regular.
- Policy Gradient.
	- REINFORCE.
		- old and bad
	- Proximal Policy Optimisation.
- Catastrophic Forgetting.
	- Is what's causing all the problems...
- NN convergence.
	- Convergence of deep convolutional neural networks, Xu 2022
	- A Convergence Theorem for Sequential Learning in Two-Layer Perceptrons, Marchand, 1990
- Random papers & resources.
	- Reinforcement Learning: An Introduction, Sutton/Barto, 1992
	- REINFORCE, Williams, 1992
	- Q-Learning, Watkins, 1989
	- Playing Atari with Deep Reinforcement Learning, ???, 2013 (?)
	- https://lilianweng.github.io/posts/2018-02-19-rl-overview/ 
	- Gym Interface: https://www.gymlibrary.dev/ 
- "Value Decomposition methods are off-policy" - https://bair.berkeley.edu/blog/2022/07/10/pg-ar/
- https://ai.stackexchange.com/questions/28079/deep-q-learning-catastrophic-drop-reasons

- "Deep Q-Learning struggles to learn unless experience replay due to the strong correlation between transitions." - source..?

Questions from https://www.youtube.com/watch?v=5Ke-d1Itk3k
	- "once we introduce a baseline [to REINFORCE] it's no longer a pure policy-based method." What? Why?
	- Disinction between REINFORCE & Actor-Critic.
DQN/TD implementations were NOT broken, hyperparameters were just bad.

w.b. 15.1.24
	- "Asynchronous Methods for Deep Reinforcement Learning" is the original A3C paper.
		- A3C is the original, A2C is the name for synchronous variants (src: https://openai.com/research/openai-baselines-acktr-a2c).
		- No paper for A2C?
		- A2C/A3C is just actor-critic with entropy. Advantage comes from "Advantage Function", which is a synthetic metric for optimizing.
			- In the Async Methods paper, Advantage(...) = Reward(...) + Entropy(...)
	- Entropy.
		- Start w/ "Function optimization using connectionist reinforcement learning algorithms" (REINFORCE/MENT paper).
w.b. 5.2.24
	- It's common practise to manually prevent agent from taking invalid actions.
- Remember to pip freeze requirements, or it might not be reproducable (src: reddit comment).